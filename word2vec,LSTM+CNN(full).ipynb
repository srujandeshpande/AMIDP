{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec,LSTM+CNN(full).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMMuqlSzZGmX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ3Dg0B4NAsq"
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from math import exp\n",
        "from numpy import sign\n",
        "\n",
        "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZrhSLHkNp3D",
        "outputId": "f279e650-17db-4553-ee49-d61fc269ac92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY-nKdOsOvEq"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTVKh-FTYba_"
      },
      "source": [
        "DEPRES_NROWS = 516088  # number of rows to read from DEPRESSIVE_TWEETS_CSV\n",
        "RANDOM_NROWS = 683702 # number of rows to read from RANDOM_TWEETS_CSV\n",
        "MAX_SEQUENCE_LENGTH = 90 # Max tweet size\n",
        "MAX_NB_WORDS = 1200000\n",
        "EMBEDDING_DIM = 300\n",
        "TRAIN_SPLIT = 0.6\n",
        "TEST_SPLIT = 0.2\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS= 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ6lTTfUP5xY"
      },
      "source": [
        "sdf=pd.read_csv('/content/gdrive/My Drive/Capstone/dataset/final_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc3I_2UdNtx_"
      },
      "source": [
        "sdf=sdf.drop(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'created_at', 'tweet',\t'language',\t'clean_tweet',\t'clean_tweet_exp',\t'cleanest_tweet','user_id'], axis = 1 )\n",
        "sdf = sdf.sample(frac = 1)\n",
        "sdf['lem_tweet']=sdf['lem_tweet'].map(str)\n",
        "tweets=sdf['lem_tweet']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfBod0HNZ2pf",
        "outputId": "0ac9bc0f-33d9-4ccd-98d6-753b2d7f7854"
      },
      "source": [
        "depressive_tweets_arr=sdf.loc[sdf['vader_sentiment_label'] == 0.0]\n",
        "depressive_tweets_arr=depressive_tweets_arr.drop(['vader_score','vader_sentiment_label'],axis = 1)\n",
        "depressive_tweets_arr.head(5)\n",
        "depressive_tweets_arr=list(depressive_tweets_arr['lem_tweet'])\n",
        "X_d=depressive_tweets_arr\n",
        "depressive_tweets_arr[0]\n",
        "len(depressive_tweets_arr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516088"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK45PV9accXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9edmTghRbWcH",
        "outputId": "49ba4d5b-3426-4bdb-9911-dfdd12a0150a"
      },
      "source": [
        "random_tweets_arr=sdf.loc[sdf['vader_sentiment_label'] == 1.0]\n",
        "random_tweets_arr=random_tweets_arr.drop(['vader_score','vader_sentiment_label'],axis = 1)\n",
        "random_tweets_arr.head(5)\n",
        "random_tweets_arr=list(random_tweets_arr['lem_tweet'])\n",
        "random_tweets_arr[0]\n",
        "X_r=random_tweets_arr\n",
        "len(random_tweets_arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "683702"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u09A84OQb35"
      },
      "source": [
        "word2vec = KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/Capstone/dataset/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk7FURQoSyDb"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_d + X_r)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HvulWtHUjSa",
        "outputId": "4702cf21-9d1c-498e-ebc2-e6e1128dfc1d"
      },
      "source": [
        "sequences_d = tokenizer.texts_to_sequences(X_d)\n",
        "sequences_r = tokenizer.texts_to_sequences(X_r)\n",
        "print(sequences_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY2LBK_lUyM5",
        "outputId": "2a40603c-9d2a-4baa-b02f-25229e102f68"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 380536 unique tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYAKHU_PVCHZ",
        "outputId": "0cc7bcfa-c67c-4e2a-e543-b309d0cbbe06"
      },
      "source": [
        "\n",
        "data_d = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_r = pad_sequences(sequences_r, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data_d tensor:', data_d.shape)\n",
        "print('Shape of data_r tensor:', data_r.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data_d tensor: (516088, 90)\n",
            "Shape of data_r tensor: (683702, 90)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdngmNg7VZqp",
        "outputId": "4f7a9fbc-5f1d-47b5-bf21-3483e75d6988"
      },
      "source": [
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words+1, EMBEDDING_DIM))\n",
        "\n",
        "print(word_index)\n",
        "for (word, idx) in word_index.items():\n",
        "    if word in word2vec.vocab and idx < MAX_NB_WORDS:\n",
        "        embedding_matrix[idx] = word2vec.word_vec(word)\n",
        "embedding_matrix[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.25585938e-01, -1.95312500e-02,  9.08203125e-02,  2.37304688e-01,\n",
              "       -2.92968750e-02,  9.32617188e-02, -5.88378906e-02, -4.10156250e-02,\n",
              "        5.22460938e-02,  2.00195312e-02, -3.44238281e-02, -2.91015625e-01,\n",
              "       -2.61718750e-01, -1.90429688e-01,  3.85742188e-02,  8.30078125e-03,\n",
              "        3.14941406e-02,  7.47070312e-02,  1.14746094e-01,  1.24511719e-02,\n",
              "       -2.11914062e-01,  1.28906250e-01,  2.55859375e-01,  1.08886719e-01,\n",
              "       -7.03125000e-02,  1.60156250e-01, -4.00390625e-01, -1.16699219e-01,\n",
              "       -3.22265625e-02, -6.73828125e-02,  1.55273438e-01,  1.25000000e-01,\n",
              "       -6.22558594e-02,  3.56445312e-02, -1.79687500e-01,  1.58203125e-01,\n",
              "       -2.53906250e-01,  1.74804688e-01, -1.18164062e-01, -2.19726562e-03,\n",
              "        1.49536133e-02, -5.71289062e-02,  2.39257812e-01,  1.36718750e-01,\n",
              "        1.47460938e-01,  7.08007812e-02,  1.72851562e-01, -2.38281250e-01,\n",
              "       -5.71289062e-02, -1.04492188e-01,  2.23388672e-02,  8.74023438e-02,\n",
              "        1.43554688e-01,  3.47656250e-01, -9.22851562e-02,  2.51953125e-01,\n",
              "       -9.86328125e-02,  1.21582031e-01, -1.56250000e-02, -1.08398438e-01,\n",
              "       -2.05078125e-02,  1.49536133e-02, -2.77343750e-01,  5.12695312e-03,\n",
              "       -9.27734375e-02, -2.17773438e-01, -1.07421875e-01,  4.93164062e-02,\n",
              "       -1.48437500e-01, -7.95898438e-02,  6.88476562e-02,  6.05468750e-02,\n",
              "       -2.72216797e-02,  7.03125000e-02, -2.11914062e-01, -1.57226562e-01,\n",
              "        8.69140625e-02, -2.81982422e-02,  9.08203125e-02,  5.63964844e-02,\n",
              "       -2.19726562e-01, -1.31835938e-02, -3.63769531e-02, -1.52343750e-01,\n",
              "       -5.06591797e-03,  1.97265625e-01,  4.61425781e-02,  2.36328125e-01,\n",
              "       -1.59179688e-01,  4.95605469e-02,  4.61425781e-02,  1.88476562e-01,\n",
              "        1.04370117e-02, -1.47094727e-02,  1.63574219e-02,  2.31445312e-01,\n",
              "       -1.73828125e-01, -2.66113281e-02,  3.00781250e-01,  9.42382812e-02,\n",
              "       -6.44531250e-02,  8.97216797e-03,  7.81250000e-02,  1.38671875e-01,\n",
              "       -3.93066406e-02,  2.27539062e-01, -1.33789062e-01,  1.36718750e-01,\n",
              "        7.71484375e-02, -1.65039062e-01, -2.53906250e-01, -2.77343750e-01,\n",
              "        6.95800781e-03, -5.55419922e-03,  2.51953125e-01,  2.57568359e-02,\n",
              "       -9.81445312e-02, -2.41210938e-01,  2.42919922e-02, -2.73437500e-01,\n",
              "        2.43164062e-01, -2.28271484e-02, -1.91406250e-01, -6.44531250e-02,\n",
              "        8.20312500e-02, -1.97265625e-01, -3.28125000e-01,  1.12304688e-01,\n",
              "        2.29492188e-01,  5.34667969e-02, -2.35351562e-01, -1.30859375e-01,\n",
              "       -1.92382812e-01, -1.13525391e-02, -2.42187500e-01, -6.15234375e-02,\n",
              "       -1.98242188e-01,  2.77099609e-02,  1.81640625e-01, -9.33837891e-03,\n",
              "        1.36718750e-01, -1.82617188e-01,  1.03027344e-01,  5.27343750e-02,\n",
              "       -5.32226562e-02,  6.74438477e-03, -1.57226562e-01,  1.12304688e-02,\n",
              "       -1.67236328e-02, -3.71093750e-02,  1.04003906e-01, -1.82617188e-01,\n",
              "       -1.36718750e-01,  6.64062500e-02, -1.73828125e-01, -2.03125000e-01,\n",
              "        4.49218750e-02,  1.54296875e-01, -7.03125000e-02,  5.78613281e-02,\n",
              "        2.11914062e-01,  1.42578125e-01, -1.71875000e-01,  1.69921875e-01,\n",
              "       -1.31835938e-01, -2.24609375e-01,  2.26562500e-01,  1.74804688e-01,\n",
              "       -2.61718750e-01,  1.00097656e-01, -3.00781250e-01, -5.24902344e-02,\n",
              "        3.23486328e-03, -9.37500000e-02,  5.02929688e-02,  2.57812500e-01,\n",
              "        4.37500000e-01, -2.75390625e-01,  2.43164062e-01, -3.16406250e-01,\n",
              "        1.20117188e-01,  8.39843750e-02,  7.28607178e-04, -2.99072266e-02,\n",
              "        6.25000000e-02,  1.28906250e-01, -1.70898438e-01,  1.98242188e-01,\n",
              "       -6.59179688e-02, -6.20117188e-02,  1.59912109e-02,  2.20947266e-02,\n",
              "       -2.81250000e-01, -1.38671875e-01,  2.55859375e-01,  2.20703125e-01,\n",
              "       -1.24023438e-01,  9.37500000e-02, -9.70458984e-03, -3.39843750e-01,\n",
              "       -2.02148438e-01,  1.24023438e-01, -5.59082031e-02, -1.76757812e-01,\n",
              "       -7.03125000e-02, -1.92382812e-01, -7.12890625e-02,  3.12500000e-02,\n",
              "       -1.38671875e-01,  2.40234375e-01,  9.08203125e-02,  2.01171875e-01,\n",
              "       -2.81250000e-01, -1.74804688e-01, -2.08007812e-01, -7.51953125e-02,\n",
              "        3.04687500e-01, -1.16210938e-01, -1.89453125e-01, -6.29882812e-02,\n",
              "       -3.18359375e-01,  1.70898438e-01,  1.61132812e-01,  1.34765625e-01,\n",
              "        2.46582031e-02, -2.85156250e-01, -1.01928711e-02,  1.21582031e-01,\n",
              "       -1.66015625e-01, -1.36718750e-01,  1.52343750e-01,  9.27734375e-02,\n",
              "        3.49121094e-02, -3.49121094e-02,  2.05078125e-01,  7.22656250e-02,\n",
              "        3.95507812e-02,  2.06054688e-01,  1.51367188e-01,  1.04980469e-01,\n",
              "        1.68945312e-01,  6.29882812e-02,  1.28906250e-01, -1.23046875e-01,\n",
              "       -2.65625000e-01, -4.05273438e-02, -2.16796875e-01,  1.78710938e-01,\n",
              "        2.30073929e-05, -1.10351562e-01,  1.95312500e-01,  1.54296875e-01,\n",
              "       -9.27734375e-02,  1.82617188e-01,  8.64257812e-02, -3.43322754e-03,\n",
              "        5.20019531e-02,  1.58691406e-02,  5.55419922e-03, -1.31835938e-01,\n",
              "        1.64062500e-01, -1.24511719e-01,  6.49414062e-02, -9.08203125e-02,\n",
              "       -1.30859375e-01,  6.07910156e-02, -1.38671875e-01, -7.66601562e-02,\n",
              "       -2.99072266e-02, -1.97265625e-01, -1.05957031e-01, -1.38671875e-01,\n",
              "        1.24023438e-01,  1.31835938e-01,  1.88476562e-01, -4.66308594e-02,\n",
              "       -1.47460938e-01, -2.07031250e-01, -2.25585938e-01,  1.12792969e-01,\n",
              "        1.39770508e-02,  1.32812500e-01,  1.69921875e-01,  1.17187500e-01,\n",
              "       -2.31933594e-02,  1.09375000e-01,  1.62109375e-01, -2.20703125e-01,\n",
              "       -1.50390625e-01, -1.92871094e-02,  7.56835938e-02,  2.25585938e-01,\n",
              "       -1.11328125e-01, -3.95507812e-02,  3.51562500e-02, -1.13281250e-01,\n",
              "       -1.76757812e-01,  2.81982422e-02, -1.77734375e-01, -6.04248047e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o-HY4GDWCBd"
      },
      "source": [
        "# Assigning labels to the depressive tweets and random tweets data\n",
        "labels_d = np.array([1] * DEPRES_NROWS)\n",
        "labels_r = np.array([0] * RANDOM_NROWS)\n",
        "\n",
        "# Splitting the arrays into test (60%), validation (20%), and train data (20%)\n",
        "perm_d = np.random.permutation(len(data_d))\n",
        "idx_train_d = perm_d[:int(len(data_d)*(TRAIN_SPLIT))]\n",
        "idx_test_d = perm_d[int(len(data_d)*(TRAIN_SPLIT)):int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
        "idx_val_d = perm_d[int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
        "\n",
        "perm_r = np.random.permutation(len(data_r))\n",
        "idx_train_r = perm_r[:int(len(data_r)*(TRAIN_SPLIT))]\n",
        "idx_test_r = perm_r[int(len(data_r)*(TRAIN_SPLIT)):int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
        "idx_val_r = perm_r[int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
        "\n",
        "# Combine depressive tweets and random tweets arrays\n",
        "data_train = np.concatenate((data_d[idx_train_d], data_r[idx_train_r]))\n",
        "labels_train = np.concatenate((labels_d[idx_train_d], labels_r[idx_train_r]))\n",
        "data_test = np.concatenate((data_d[idx_test_d], data_r[idx_test_r]))\n",
        "labels_test = np.concatenate((labels_d[idx_test_d], labels_r[idx_test_r]))\n",
        "data_val = np.concatenate((data_d[idx_val_d], data_r[idx_val_r]))\n",
        "labels_val = np.concatenate((labels_d[idx_val_d], labels_r[idx_val_r]))\n",
        "\n",
        "# Shuffling\n",
        "perm_train = np.random.permutation(len(data_train))\n",
        "data_train = data_train[perm_train]\n",
        "labels_train = labels_train[perm_train]\n",
        "perm_test = np.random.permutation(len(data_test))\n",
        "data_test = data_test[perm_test]\n",
        "labels_test = labels_test[perm_test]\n",
        "perm_val = np.random.permutation(len(data_val))\n",
        "data_val = data_val[perm_val]\n",
        "labels_val = labels_val[perm_val]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm4F6bYlXvcW"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "# Embedded layer\n",
        "model.add(Embedding(len(embedding_matrix), EMBEDDING_DIM, weights=[embedding_matrix], \n",
        "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "# Convolutional Layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "# LSTM Layer\n",
        "model.add(LSTM(300))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiouYnEgYVxD",
        "outputId": "a9631277-ad46-4544-9308-002f1036b629"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 90, 300)           114161100 \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 90, 32)            28832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 45, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 45, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300)               399600    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 114,589,833\n",
            "Trainable params: 428,733\n",
            "Non-trainable params: 114,161,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "fEoXDRFYdtsO",
        "outputId": "36d7a129-6bba-4923-93d7-c745818e8345"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=1000)\n",
        "\n",
        "hist = model.fit(data_train, labels_train, \\\n",
        "        validation_data=(data_val, labels_val), \\\n",
        "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
        "        callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5ceeeb2b78a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01EICzsmh0bM"
      },
      "source": [
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwwMjPOeim7k"
      },
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgnhitFzi2Hi"
      },
      "source": [
        "labels_pred = model.predict(data_test)\n",
        "labels_pred = np.round(labels_pred.flatten())\n",
        "accuracy = accuracy_score(labels_test, labels_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy*100))\n",
        "type(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jjC5Nx9i6du"
      },
      "source": [
        "print(classification_report(labels_test, labels_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHQwoI5LYrpd"
      },
      "source": [
        "model.save('/content/gdrive/MyDrive/Capstone/model/model1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSf7_BJbjCE8"
      },
      "source": [
        "class LogReg:\n",
        "    \"\"\"\n",
        "    Class to represent a logistic regression model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, l_rate, epochs, n_features):\n",
        "        \"\"\"\n",
        "        Create a new model with certain parameters.\n",
        "\n",
        "        :param l_rate: Initial learning rate for model.\n",
        "        :param epoch: Number of epochs to train for.\n",
        "        :param n_features: Number of features.\n",
        "        \"\"\"\n",
        "        self.l_rate = l_rate\n",
        "        self.epochs = epochs\n",
        "        self.coef = [0.0] * n_features\n",
        "        self.bias = 0.0\n",
        "\n",
        "    def sigmoid(self, score, threshold=20.0):\n",
        "        \"\"\"\n",
        "        Prevent overflow of exp by capping activation at 20.\n",
        "\n",
        "        :param score: A real valued number to convert into a number between 0 and 1\n",
        "        \"\"\"\n",
        "        if abs(score) > threshold:\n",
        "            score = threshold * sign(score)\n",
        "        activation = exp(score)\n",
        "        return activation / (1.0 + activation)\n",
        "\n",
        "    def predict(self, features):\n",
        "        \"\"\"\n",
        "        Given an example's features and the coefficients, predicts the class.\n",
        "\n",
        "        :param features: List of real valued features for a single training example.\n",
        "\n",
        "        :return: Returns the predicted class (either 0 or 1).\n",
        "        \"\"\"\n",
        "        value = sum([features[i]*self.coef[i] for i in range(len(features))]) + self.bias\n",
        "        return self.sigmoid(value)\n",
        "\n",
        "    def sg_update(self, features, label):\n",
        "        \"\"\"\n",
        "        Computes the update to the weights based on a predicted example.\n",
        "\n",
        "        :param features: Features to train on.\n",
        "        :param label: Corresponding label for features.\n",
        "        \"\"\"\n",
        "        yhat = self.predict(features)\n",
        "        e = label - yhat\n",
        "        self.bias = self.bias + self.l_rate * e * yhat * (1-yhat)\n",
        "        for i in range(len(features)):\n",
        "            self.coef[i] = self.coef[i] + self.l_rate * e * yhat * (1-yhat) * features[i]\n",
        "        return\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes logistic regression coefficients using stochastic gradient descent.\n",
        "\n",
        "        :param X: Features to train on.\n",
        "        :param y: Corresponding label for each set of features.\n",
        "\n",
        "        :return: Returns a list of model weight coefficients where coef[0] is the bias.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            for features, label in zip(X, y):\n",
        "                self.sg_update(features, label)\n",
        "        return self.bias, self.coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiIzeGeIjeIF"
      },
      "source": [
        "def get_accuracy(y_bar, y_pred):\n",
        "    \"\"\"\n",
        "    Computes what percent of the total testing data the model classified correctly.\n",
        "\n",
        "    :param y_bar: List of ground truth classes for each example.\n",
        "    :param y_pred: List of model predicted class for each example.\n",
        "\n",
        "    :return: Returns a real number between 0 and 1 for the model accuracy.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for i in range(len(y_bar)):\n",
        "        if y_bar[i] == y_pred[i]:\n",
        "            correct += 1\n",
        "    accuracy = (correct / len(y_bar)) * 100.0\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sdb2IgwjFs9"
      },
      "source": [
        "logreg = LogReg(LEARNING_RATE, EPOCHS, len(data_train[0]))\n",
        "bias_logreg, weights_logreg = logreg.train(data_train, labels_train)\n",
        "y_logistic = [round(logreg.predict(example)) for example in data_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ViD5uyijImk"
      },
      "source": [
        "accuracy_logistic = get_accuracy(y_logistic, labels_test)\n",
        "print('Logistic Regression Accuracy: {:0.3f}'.format(accuracy_logistic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QyH7b9IjOF4"
      },
      "source": [
        "test=pd.read_csv('/content/gdrive/MyDrive/Capstone/dataset/created_depression.csv')\n",
        "test.head()\n",
        "a=test['Tweet'].astype('string')\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(a)\n",
        "sequences_d = tokenizer.texts_to_sequences(a)\n",
        "word_index = tokenizer.word_index\n",
        "data_d = pad_sequences(sequences_d, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "print(nb_words)\n",
        "print(word_index)\n",
        "print(MAX_NB_WORDS)\n",
        "for (word, idx) in word_index.items():\n",
        "  if word in word2vec.vocab and idx < MAX_NB_WORDS:\n",
        "    embedding_matrix[idx-1] = word2vec.word_vec(word)\n",
        "test = data_d\n",
        "labels_pred=model.predict(test)\n",
        "print(labels_pred)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEdMVEefj1cv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}